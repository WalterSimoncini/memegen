{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "This notebook trains a meme generation model based on: https://github.com/dylanwenzlau/ml-scripts/blob/master/meme_text_gen_convnet/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIEjTZeZ3r4e"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \".\"\n",
    "TRAINING_DATA_PATH = f\"{BASE_PATH}/drake_hotline_bling.json\"\n",
    "SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 16\n",
    "NUM_EPOCHS = 60\n",
    "BATCH_SIZE = 256\n",
    "VAL_RATIO = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Al4Kzv-D3tzq"
   },
   "outputs": [],
   "source": [
    "memes = json.loads(open(TRAINING_DATA_PATH).read())['memes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nh6Opt_t3zM5"
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels_vocabulary = {}\n",
    "current_labels_vocabulary_key = 0\n",
    "labels = []\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "for i, meme in enumerate(memes):\n",
    "    box_index = 0\n",
    "    meme_id = \"1\".zfill(8)\n",
    "    meme_text = f\"{meme['text']};\"\n",
    "\n",
    "    if not is_ascii(meme_text):\n",
    "        continue\n",
    "\n",
    "    for j in range(1, len(meme_text)):\n",
    "        character = meme_text[j]\n",
    "        texts.append(f\"{meme_id} {box_index} {meme_text[0:j]}\")\n",
    "\n",
    "        if character not in labels_vocabulary:\n",
    "            labels_vocabulary[character] = current_labels_vocabulary_key\n",
    "            current_labels_vocabulary_key += 1\n",
    "\n",
    "        character_label = labels_vocabulary[character]\n",
    "        labels.append(character_label)\n",
    "\n",
    "        if character == \";\":\n",
    "          box_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7j0pFuz31UF"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Tokenization\n",
    "\"\"\"\n",
    "tk = Tokenizer(num_words=0, char_level=True)\n",
    "tk.fit_on_texts(texts)\n",
    "\n",
    "vocabulary = tk.word_index\n",
    "sequences = tk.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-r4GwXz32Vy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prepare the sequences and split the dataset in training and validation\n",
    "\"\"\"\n",
    "data = pad_sequences(sequences, maxlen=SEQUENCE_LENGTH)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "val_samples_count = int(VAL_RATIO * data.shape[0])\n",
    "training_samples_count = data.shape[0] - val_samples_count\n",
    "\n",
    "X_train = data[:training_samples_count]\n",
    "y_train = labels[:training_samples_count]\n",
    "\n",
    "X_val = data[-val_samples_count:]\n",
    "y_val = labels[-val_samples_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4KnOKEwDGA0"
   },
   "outputs": [],
   "source": [
    "with open(f\"{BASE_PATH}/params.json\", \"w\") as params_file:\n",
    "  params_file.write(json.dumps({\n",
    "      'sequence_length': SEQUENCE_LENGTH,\n",
    "      'embedding_dim': EMBEDDING_DIM,\n",
    "      'samples_count': len(memes),\n",
    "      'num_epochs': NUM_EPOCHS,\n",
    "      'batch_size': BATCH_SIZE,\n",
    "      'vocabulary': vocabulary,\n",
    "      'labels_vocabulary': labels_vocabulary\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "pUMaQeP235UM",
    "outputId": "2329a887-d543-4804-ee98-fc9500d49446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 16)           1008      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 1024)         82944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 1024)         4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 64, 1024)          5243904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 1024)          5243904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 62)                63550     \n",
      "=================================================================\n",
      "Total params: 11,701,294\n",
      "Trainable params: 11,693,102\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Setup the model\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(len(vocabulary) + 1, EMBEDDING_DIM, input_length=SEQUENCE_LENGTH))\n",
    "model.add(layers.Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Conv1D(1024, 5, activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(len(labels_vocabulary), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sYl5a7jD-fg"
   },
   "outputs": [],
   "source": [
    "# Create a directory for saved models\n",
    "!mkdir out_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLOgAqFk36zy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Setup checkpoints\n",
    "\"\"\"\n",
    "checkpointer = ModelCheckpoint(filepath='out_models/model.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPerDTjX38ay"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Train\n",
    "\"\"\"\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpointer])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MemeV2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
